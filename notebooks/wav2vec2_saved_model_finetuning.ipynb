{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('t5': conda)"
    },
    "interpreter": {
      "hash": "184c0bf4d405d4a36e719b504ff2a22c838d19108535bf816dff1a5aad495b87"
    },
    "colab": {
      "name": "wav2vec2-saved-model-finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "rWk8nL6Ui-_0",
        "wvuJL8-f0zn5",
        "oPp18ZHRtnq-",
        "1mvTuOXpwsQe",
        "7Vlm3ySFULsG",
        "SJtSLgCvxvBG",
        "SJfPlTgezD0i"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/export-v2/notebooks/wav2vec2_saved_model_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# How to train TensorFlow saved-model with extra head\n",
        "\n",
        "In this notebook, we will load the pre-trained wav2vec2 model from [TFHub](https://tfhub.dev) and will train it on [LibriSpeech dataset](https://huggingface.co/datasets/librispeech_asr) by appending LM head over the top of our pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Setting Up\n",
        "\n",
        "Before diving into it, let's see what GPU we got using `nvidia-smi`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2DQV2hde_Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa9ee5b6-beab-4374-d68d-1dc47420a244"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 20 23:20:53 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hBGUWT_mKkw"
      },
      "source": [
        "The following cell will clone my code repository ([`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)) and will install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seqTlMyeZvM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87ee5424-3696-4042-aacc-6a4c7ca98648"
      },
      "source": [
        "!git clone https://github.com/vasudevgupta7/gsoc-wav2vec2 --branch=export-v2\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir(\"gsoc-wav2vec2\")\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "!pip3 install -qe ."
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gsoc-wav2vec2'...\n",
            "remote: Enumerating objects: 389, done.\u001b[K\n",
            "remote: Counting objects: 100% (389/389), done.\u001b[K\n",
            "remote: Compressing objects: 100% (245/245), done.\u001b[K\n",
            "remote: Total 389 (delta 205), reused 308 (delta 134), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (389/389), 2.87 MiB | 31.97 MiB/s, done.\n",
            "Resolving deltas: 100% (205/205), done.\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 32.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 65.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 61.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 64.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.2 MB/s \n",
            "\u001b[?25h  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQDM1t8Z_XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02d48453-8b60-45b7-dc77-9c8f7c5c6766"
      },
      "source": [
        "# This cell will be removed after model get exported to TFHub\n",
        "!wget https://huggingface.co/vasudevgupta/tf-wav2vec2-base/resolve/main/wav2vec2-base.tar.gz\n",
        "!tar -xf wav2vec2-base.tar.gz"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 23:41:50--  https://huggingface.co/vasudevgupta/tf-wav2vec2-base/resolve/main/wav2vec2-base.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 15.197.130.34\n",
            "Connecting to huggingface.co (huggingface.co)|15.197.130.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/vasudevgupta/tf-wav2vec2-base/6dc0b96ec02586bcf863f5368b4c2a9ef05d924625c7e16708eec037577bb072 [following]\n",
            "--2021-07-20 23:41:50--  https://cdn-lfs.huggingface.co/vasudevgupta/tf-wav2vec2-base/6dc0b96ec02586bcf863f5368b4c2a9ef05d924625c7e16708eec037577bb072\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 54.192.101.124, 54.192.101.41, 54.192.101.42, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|54.192.101.124|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 224929720 (215M) [application/x-gzip]\n",
            "Saving to: ‘wav2vec2-base.tar.gz’\n",
            "\n",
            "wav2vec2-base.tar.g 100%[===================>] 214.51M  23.0MB/s    in 7.8s    \n",
            "\n",
            "2021-07-20 23:41:59 (27.4 MB/s) - ‘wav2vec2-base.tar.gz’ saved [224929720/224929720]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Model setup using `TFHub`\n",
        "\n",
        "We will start by importing all the important libraries & modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuVyJshArx9Q"
      },
      "source": [
        "We will be loading the pre-trained saved-model directly from TFHub. [`hub.load(...)`](https://www.tensorflow.org/hub/api_docs/python/hub/load) will download the pre-trained model first and will call [`tf.saved_model.load(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load) over those downloaded weights."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5cjb_EZvM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c75f700-b1c2-40ce-d4e5-1b68548cd3b7"
      },
      "source": [
        "# TODO: update it to load from TFHub later\n",
        "loaded = hub.load(\"saved-model\")\n",
        "print(\"Available signatures are:\", list(loaded.signatures.keys()))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available signatures are: ['wav2vec2']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "We can see available model signatures above (these signatures were passed while saving model with [`tf.saved_model.save(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save) (you can refer this [script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py)). We will be using the `wav2vec2` signature in this notebook. \n",
        "\n",
        "First, we will wrap our model signature with [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to be able to use this model like any other keras layer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "source": [
        "pretrained_layer = hub.KerasLayer(loaded.signatures[\"wav2vec2\"], trainable=True)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7IwrLqbbkw1"
      },
      "source": [
        "Object `pretrained_layer` is the freezed version of [`Wav2Vec2Model`](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/wav2vec2/modeling.py). Pre-trained weights are converted from HuggingFace PyTorch [pre-trained weights](https://huggingface.co/facebook/wav2vec2-base) using [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/convert_torch_to_tf.py).\n",
        "\n",
        "Originally, wav2vec2 was pre-trained with masked language modeling approach with the objective to identify the true quantized latent speech representation for a masked time step. You can read more about training objective in the paper- [wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations](https://arxiv.org/abs/2006.11477)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SseDnCr7hyhC"
      },
      "source": [
        "Now, we will be defining few constants and hyper-parameters which will be useful in next few cells. `AUDIO_MAXLEN` is intentionally set to `246000` as the model signature only accepts static sequence length of `246000`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eiILuMBERxlO"
      },
      "source": [
        "AUDIO_MAXLEN = 246000\n",
        "LABEL_MAXLEN = 256\n",
        "BATCH_SIZE = 2"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1V4gTgGLgXvO"
      },
      "source": [
        "In following cell, we will wrap `pretrained_layer` & a dense layer (LM head) with the [TensorFlow's Functional API](https://www.tensorflow.org/guide/keras/functional)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a3CUN1KEB10Q"
      },
      "source": [
        "inputs = tf.keras.Input(shape=(AUDIO_MAXLEN,))\n",
        "hidden_states = pretrained_layer(inputs)[\"output_0\"]\n",
        "hidden_states = tf.keras.layers.Dropout(config.dropout)(hidden_states)\n",
        "outputs = tf.keras.layers.Dense(config.vocab_size)(hidden_states)\n",
        "\n",
        "model = tf.keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zDXuoMXhDMo"
      },
      "source": [
        "Dense layer (defined above) is having an output dimension of `vocab_size` as we want to predict probabilities of each token in the vocabulary at each time step."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "## Setting up training state\n",
        "\n",
        "Alright, let's define our training forward pass by calling the model with `training=True` and wrapping it with `tf.function(...)`. It's important to wrap it with `tf.function(...)` to be able to get performance benefits during training.\n",
        "\n",
        "Additionally, we will be passing `jit_compile=True` to compile (using XLA) our model graph on the accelerators (i.e GPUs/TPUs) & fuse many operations to get out-of-box performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5M8YXXZvM_"
      },
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def forward(batch):\n",
        "    return model(batch, training=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "In TensorFlow, model weights are build only when `model.__call__` is called for the first time, so the following cell will build the model weights for us. Further, we will be running `model.summary()` for checking the total number of trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgL5wyaXZvM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4fc6eabe-6c5d-4461-cbd7-2b7946929125"
      },
      "source": [
        "forward(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         [(None, 246000)]          0         \n",
            "_________________________________________________________________\n",
            "keras_layer_1 (KerasLayer)   multiple                  94370944  \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 768, 768)          0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 768, 32)           24608     \n",
            "=================================================================\n",
            "Total params: 94,395,552\n",
            "Trainable params: 24,608\n",
            "Non-trainable params: 94,370,944\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Now, we need to define `loss_fn` and optimizer to be able to train the model. The following cell will do that for us. We will be using the `Adam` optimizer for simplicity. `CTCLoss` is a very common loss type that is used for tasks (like `ASR`) where input sub-parts can't be easily aligned with output sub-parts. You can read more about CTC-loss from this amazing [blog post](https://distill.pub/2017/ctc/).\n",
        "\n",
        "\n",
        "`CTCLoss` (from [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) package) accepts 3 arguments: `config`, `model_input_shape` & `division_factor`. If `division_factor=1`, then loss will simply get summed, so pass `division_factor` accordingly to get mean over batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "LEARNING_RATE = 5e-5\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Loading & Pre-processing data\n",
        "\n",
        "Let's now download the LibriSpeech dataset from the [official website](http://www.openslr.org/12) and set it up."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4kIEC77cBCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e78a6baf-5f1a-4508-dae1-f03b364fe1ec"
      },
      "source": [
        "!wget https://www.openslr.org/resources/12/dev-clean.tar.gz -P ./data/train/\n",
        "!tar -xf ./data/train/dev-clean.tar.gz -C ./data/train/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 23:22:17--  https://www.openslr.org/resources/12/dev-clean.tar.gz\n",
            "Resolving www.openslr.org (www.openslr.org)... 46.101.158.64\n",
            "Connecting to www.openslr.org (www.openslr.org)|46.101.158.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 337926286 (322M) [application/x-gzip]\n",
            "Saving to: ‘./data/train/dev-clean.tar.gz’\n",
            "\n",
            "dev-clean.tar.gz    100%[===================>] 322.27M  20.8MB/s    in 17s     \n",
            "\n",
            "2021-07-20 23:22:35 (18.9 MB/s) - ‘./data/train/dev-clean.tar.gz’ saved [337926286/337926286]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsQpmpn6jrMI"
      },
      "source": [
        "**Note:** We are using `dev-clean` configuration as this notebook is just for demonstration purposes, so we just need small data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynxAjtGHGFpM",
        "outputId": "2d6f1b33-5730-4fe1-d4e6-7f871faaf1e3"
      },
      "source": [
        "ls ./data/train/"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dev-clean.tar.gz  \u001b[0m\u001b[01;34mLibriSpeech\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "Our dataset lies in `LibriSpeech` directory. Let's further narrow down & choose a sub-directory to see few files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIu_Wt4ZvNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b23a03e5-943e-46b9-d222-1f84dc86d010"
      },
      "source": [
        "data_dir = \"./data/train/LibriSpeech/dev-clean/2428/83705/\"\n",
        "all_files = os.listdir(data_dir)\n",
        "\n",
        "flac_files = [f for f in all_files if f.endswith(\".flac\")]\n",
        "txt_files = [f for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "print(\"Transcription files:\", txt_files, \"\\nSound files:\", flac_files)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transcription files: ['2428-83705.trans.txt'] \n",
            "Sound files: ['2428-83705-0022.flac', '2428-83705-0013.flac', '2428-83705-0005.flac', '2428-83705-0020.flac', '2428-83705-0024.flac', '2428-83705-0037.flac', '2428-83705-0012.flac', '2428-83705-0009.flac', '2428-83705-0036.flac', '2428-83705-0028.flac', '2428-83705-0006.flac', '2428-83705-0035.flac', '2428-83705-0034.flac', '2428-83705-0043.flac', '2428-83705-0017.flac', '2428-83705-0021.flac', '2428-83705-0033.flac', '2428-83705-0040.flac', '2428-83705-0010.flac', '2428-83705-0008.flac', '2428-83705-0016.flac', '2428-83705-0004.flac', '2428-83705-0029.flac', '2428-83705-0007.flac', '2428-83705-0026.flac', '2428-83705-0001.flac', '2428-83705-0025.flac', '2428-83705-0018.flac', '2428-83705-0030.flac', '2428-83705-0002.flac', '2428-83705-0015.flac', '2428-83705-0039.flac', '2428-83705-0000.flac', '2428-83705-0019.flac', '2428-83705-0038.flac', '2428-83705-0027.flac', '2428-83705-0032.flac', '2428-83705-0031.flac', '2428-83705-0011.flac', '2428-83705-0042.flac', '2428-83705-0003.flac', '2428-83705-0014.flac', '2428-83705-0023.flac', '2428-83705-0041.flac']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEObi_Apk3ZD"
      },
      "source": [
        "Alright, so each sub-directory is having many `.flac` files and single `.txt` file. `.txt` file will have text transcriptions for all the speech samples (i.e. `.flac` files) present in that sub-directory."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYW6WKJflO2e"
      },
      "source": [
        "In following cell, we will define function for loading & formatting the text data into memory."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cEBKxQblHPwq"
      },
      "source": [
        "def read_txt_file(f):\n",
        "  with open(f, \"r\") as f:\n",
        "    samples = f.read().split(\"\\n\")\n",
        "    samples = {s.split()[0]: \" \".join(s.split()[1:]) for s in samples if len(s.split()) > 2}\n",
        "  return samples"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ldkf_ceb0_YW"
      },
      "source": [
        "Similary, we will define a function for loading speech sample from `.flac` file.\n",
        "\n",
        "`REQUIRED_SAMPLE_RATE` is set to `16000` as wav2vec2 was pre-trained with `16K` frequency and it's recommended to train it further without any major change in data distribution due to frequency."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOJ3OzPsTyXv"
      },
      "source": [
        "import soundfile as sf\n",
        "\n",
        "REQUIRED_SAMPLE_RATE = 16000\n",
        "\n",
        "def read_flac_file(file_path):\n",
        "  with open(file_path, \"rb\") as f:\n",
        "      audio, sample_rate = sf.read(f)\n",
        "  if sample_rate != REQUIRED_SAMPLE_RATE:\n",
        "      raise ValueError(\n",
        "          f\"sample rate (={sample_rate}) of your files must be {REQUIRED_SAMPLE_RATE}\"\n",
        "      )\n",
        "  file_id = os.path.split(file_path)[-1][:-len(\".flac\")]\n",
        "  return {file_id: audio}"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8jJ7Ed81p_A"
      },
      "source": [
        "Now, we will combine all the speech & text samples and will define the function (in next cell) for that purpose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI-5YCzaTsei"
      },
      "source": [
        "def fetch_sound_text_mapping(data_dir):\n",
        "  all_files = os.listdir(data_dir)\n",
        "\n",
        "  flac_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".flac\")]\n",
        "  txt_files = [os.path.join(data_dir, f) for f in all_files if f.endswith(\".txt\")]\n",
        "\n",
        "  txt_samples = {}\n",
        "  for f in txt_files:\n",
        "    txt_samples.update(read_txt_file(f))\n",
        "\n",
        "  speech_samples = {}\n",
        "  for f in flac_files:\n",
        "    speech_samples.update(read_flac_file(f))\n",
        "\n",
        "  assert len(txt_samples) == len(speech_samples)\n",
        "\n",
        "  samples = [(txt_samples[file_id], speech_samples[file_id]) for file_id in speech_samples.keys()]\n",
        "  return samples"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx95Lxvu0nT4"
      },
      "source": [
        "It's time to have a look at a few samples ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Ls7X_jqIz4R",
        "outputId": "6e9c520a-bf9e-4777-b625-8d3ce59e77e0"
      },
      "source": [
        "samples = fetch_sound_text_mapping(data_dir)\n",
        "samples[:5]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(\"A BIRD IN THE HAND IS WORTH TWO IN A BUSH' AND IT WILL BE SOMETHING TO HAVE BY US\",\n",
              "  array([0.00085449, 0.00073242, 0.0005188 , ..., 0.00048828, 0.00054932,\n",
              "         0.0005188 ])),\n",
              " ('IT IS FROM HER ACTION IN THAT MATTER THAT MY SUSPICION SPRINGS',\n",
              "  array([-0.0007019 , -0.00057983, -0.00033569, ..., -0.00021362,\n",
              "         -0.00015259, -0.00012207])),\n",
              " ('FOR INSTANCE LOOK AT THEIR BEHAVIOUR IN THE MATTER OF THE RING',\n",
              "  array([-0.00201416, -0.0022583 , -0.00234985, ...,  0.00137329,\n",
              "          0.0012207 ,  0.00109863])),\n",
              " ('IT WAS PLAIN THAT TOGETHER WE SHOULD MANAGE MOST COMFORTABLY DELIGHTFULLY IN FACT',\n",
              "  array([-9.15527344e-05,  9.15527344e-05, -1.83105469e-04, ...,\n",
              "         -5.79833984e-04, -4.88281250e-04, -3.96728516e-04])),\n",
              " ('AND I WILL SEE THAT THERE IS NO SHIRKING ABOUT THE BOYS OR ABOUT THE GIRLS EITHER',\n",
              "  array([-1.49536133e-03, -9.76562500e-04, -3.05175781e-05, ...,\n",
              "         -2.13623047e-04, -2.74658203e-04, -1.83105469e-04]))]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xg8Zia1kzw0J"
      },
      "source": [
        "Let's pre-process the data now !!!\n",
        "\n",
        "We will first define the tokenizer & processor using gsoc-wav2vec2 package.Then, we will do very simple pre-processing. Speech will be normalized over time axis and text will be tokenized using `processor` and `tokenizer` respectively."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gaat_hMLNVHF",
        "outputId": "eeb50ebe-2fb5-414e-f91d-33c31844bef3"
      },
      "source": [
        "from wav2vec2 import Wav2Vec2Processor\n",
        "tokenizer = Wav2Vec2Processor(is_tokenizer=True)\n",
        "processor = Wav2Vec2Processor(is_tokenizer=False)\n",
        "\n",
        "def preprocess_text(text):\n",
        "  label = tokenizer(text)\n",
        "  return tf.constant(label, dtype=tf.int32)\n",
        "\n",
        "def preprocess_speech(audio):\n",
        "  audio = tf.constant(audio, dtype=tf.float32)\n",
        "  return processor(tf.transpose(audio))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading `vocab.json` from https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/vocab.json ... DONE\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyKl8QP-zRFC"
      },
      "source": [
        "Now, we will define the python generator to call the preprocessing functions we defined in above cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoQrRalwMpQ6"
      },
      "source": [
        "def inputs_generator(samples):\n",
        "  for text, speech in samples:\n",
        "    yield preprocess_text(text), preprocess_speech(speech)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8GruDUhROZwD",
        "outputId": "8afc5d48-a871-46ec-8541-3c9577410f02"
      },
      "source": [
        "from functools import partial\n",
        "generator = partial(inputs_generator, samples=samples)\n",
        "next(iter(generator()))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<tf.Tensor: shape=(81,), dtype=int32, numpy=\n",
              " array([ 7,  4, 24, 10, 13, 14,  4, 10,  9,  4,  6, 11,  5,  4, 11,  7,  9,\n",
              "        14,  4, 10, 12,  4, 18,  8, 13,  6, 11,  4,  6, 18,  8,  4, 10,  9,\n",
              "         4,  7,  4, 24, 16, 12, 11, 27,  4,  7,  9, 14,  4, 10,  6,  4, 18,\n",
              "        10, 15, 15,  4, 24,  5,  4, 12,  8, 17,  5,  6, 11, 10,  9, 21,  4,\n",
              "         6,  8,  4, 11,  7, 25,  5,  4, 24, 22,  4, 16, 12], dtype=int32)>,\n",
              " <tf.Tensor: shape=(88400,), dtype=float32, numpy=\n",
              " array([0.01726047, 0.01499128, 0.0110202 , ..., 0.01045291, 0.0115875 ,\n",
              "        0.0110202 ], dtype=float32)>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Vlm3ySFULsG"
      },
      "source": [
        "## Setting up `tf.data.Dataset`\n",
        "\n",
        "Following cell will setup `tf.data.Dataset` object using its `.from_generator(...)` method. We will be using the `generator` object, we defined in above cell.\n",
        "\n",
        "**Note:** For distributed training (especially on TPUs), `.from_generator(...)` doesn't work currently & its recommended to train on data stored in `.tfrecord` format. You can refer [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) for more details on how to convert LibriSpeech data into tfrecords."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LbQ_dMwGO62h"
      },
      "source": [
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None), dtype=tf.int32),\n",
        "    tf.TensorSpec(shape=(None),  dtype=tf.float32),\n",
        ")\n",
        "dataset = tf.data.Dataset.from_generator(generator, output_signature=output_signature)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1eDBZjkogsG"
      },
      "source": [
        "Let's shuffle the dataset using `.shuffle(...)` method. Argument buffer size leads to approximate shuffling as many times complete dataset can't be fitted into memory for actual shuffling (Eg. complete LibriSpeech tfrecords takes around 250 GB on disk)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXBbNsRyPyw3"
      },
      "source": [
        "BUFFER_SIZE = len(flac_files)\n",
        "SEED = 42\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE, seed=SEED)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DAUmns3pXfr"
      },
      "source": [
        "We will pass dataset into multiple batches, so let's prepare batches in following cell. Now, all the sequences in a batch should be padded to constant length. We will use `.padded_batch(...)` method for that purpose. We also need to restrict sequence length to some particular value as some of the sequences are very long.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Okhko1IWRida"
      },
      "source": [
        "dataset = dataset.map(lambda labels, speech: (labels[: LABEL_MAXLEN], speech[: AUDIO_MAXLEN]))\n",
        "dataset = dataset.padded_batch(BATCH_SIZE, padded_shapes=(LABEL_MAXLEN, AUDIO_MAXLEN), padding_values=(0, 0.))"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A45CjQG5qSbV"
      },
      "source": [
        "Accelerators (like GPUs/TPUs) are very fast and often data-loading (& pre-processing) becomes the bottle-neck during training as data-loading part happens on CPUs. This can increase the training time significantly especially when there is lot of online pre-processing involved or data is streamed online from GCS buckets. To handle those issues, `tf.data.Dataset` offers the `.prefetch(...)` method. This method helps in preparing next few batches in parallel (on CPUs) while model is making predictions (on GPUs/TPUs) on the current batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-bKu2YjRior"
      },
      "source": [
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Since this notebook is made for demonstration purposes, we will be taking first `num_batches` and will perform training over only that. You are encouraged to train on whole dataset though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "source": [
        "num_batches = 2\n",
        "dataset = dataset.take(num_batches)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJtSLgCvxvBG"
      },
      "source": [
        "## Training\n",
        "\n",
        "Let's define our `train_step` function now. There are 3 main steps in `train_step`: \n",
        "1. forward pass with variables tracking\n",
        "2. backward pass for calculating gradients\n",
        "3. variables update to minimize training loss\n",
        "\n",
        "All the trainable variables in the scope of `tf.GradientTape(...)` will get tracked during the forward pass. Further, `.gradient(...)` will help us find gradient of loss w.r.to those tracked variables & `.apply_gradients(...)` will update the trainable variables based on our `optimizer` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RYI8ra_ZvNA"
      },
      "source": [
        "@tf.function\n",
        "def train_step(speech, labels):\n",
        "    with tf.GradientTape() as gtape:\n",
        "        speech = forward(speech)\n",
        "        loss = loss_fn(labels, speech)\n",
        "    grads = gtape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOuhcSPkytaN"
      },
      "source": [
        "Let's kick start training finally !!!\n",
        "\n",
        "We will iterate over our dataset (instance of `tf.data.Dataset`) and each batch will be fed to `train_step(...)` for calculating loss, gradients & updating parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUujvZn4ZvNA"
      },
      "source": [
        "from tqdm import tqdm\n",
        "EPOCHS = 10\n",
        "\n",
        "for e in range(1, 1+EPOCHS):\n",
        "  pbar = tqdm(dataset, total=num_batches, desc=f\"running epoch-{e}\")\n",
        "  running_loss, steps = tf.constant(0.), 0\n",
        "  for labels, speech in pbar:\n",
        "      loss = train_step(speech, labels)\n",
        "      running_loss += loss\n",
        "      steps += 1\n",
        "      pbar.set_postfix(tr_loss=running_loss/steps, batch_loss=loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "## Evaluation\n",
        "\n",
        "Let's compute loss over validation dataset using `eval_step(...)` defined in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def eval_fwd(batch):\n",
        "  return model(batch, training=False)\n",
        "\n",
        "@tf.function\n",
        "def eval_step(speech, labels):\n",
        "    speech = eval_fwd(speech)\n",
        "    loss = loss_fn(labels, speech)\n",
        "    return loss, tf.argmax(speech, axis=-1)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Io_91Y7-r3xu"
      },
      "source": [
        "We need to compute `WER` (word error rate) over our validation data. We will use `load_metric(...)` function from [HuggingFace datasets](https://huggingface.co/docs/datasets/) library. Let's first install the `datasets` library using `pip` and then define the `metric` object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GW9F_oVDU1TZ",
        "outputId": "7701aedc-a290-42b8-dfa5-66a84f2ada77"
      },
      "source": [
        "!pip3 install -q datasets\n",
        "\n",
        "from datasets import load_metric\n",
        "metric = load_metric(\"wer\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 262 kB 8.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 118 kB 13.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 243 kB 17.1 MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFh1myg1x4ua"
      },
      "source": [
        "It's time to run the evaluation on validation data now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTFVjZghckJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ed7e64a-d74a-4417-f418-5dbcb4d306d6"
      },
      "source": [
        "pbar = tqdm(dataset, total=num_batches)\n",
        "for labels, speech in pbar:\n",
        "    loss, predictions = eval_step(speech, labels)\n",
        "    pbar.set_postfix(val_loss=loss)\n",
        "    predictions = [tokenizer.decode(pred) for pred in predictions.numpy().tolist()]\n",
        "    references = [tokenizer.decode(label, group_tokens=False) for label in labels.numpy().tolist()]\n",
        "    metric.add_batch(references=references, predictions=predictions)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "  0%|          | 0/2 [00:00<?, ?it/s]\u001b[A\n",
            "  0%|          | 0/2 [00:06<?, ?it/s, val_loss=tf.Tensor(1846.5251, shape=(), dtype=float32)]\u001b[A\n",
            " 50%|█████     | 1/2 [00:06<00:06,  6.95s/it, val_loss=tf.Tensor(1846.5251, shape=(), dtype=float32)]\u001b[A\n",
            " 50%|█████     | 1/2 [00:08<00:06,  6.95s/it, val_loss=tf.Tensor(1795.4788, shape=(), dtype=float32)]\u001b[A\n",
            "100%|██████████| 2/2 [00:10<00:00,  5.12s/it, val_loss=tf.Tensor(1795.4788, shape=(), dtype=float32)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWCc8qBesv3e"
      },
      "source": [
        "We are using `tokenizer.decode(...)` method for decoding our predictions and labels back into text and will add them to metric for `WER` computation later.\n",
        "\n",
        "**Note:** We are using the same dataset just for demonstration purposes. In general, we should use separate data (generally called `validation/dev` data) sampled before initiating training."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XI_URj8Wtb2g"
      },
      "source": [
        "`metirc.compute()` will calculate the final WER score over all the batches added in previous cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a83wekLgWMod",
        "outputId": "62e468db-eda1-46f5-a0f3-62fac88c3452"
      },
      "source": [
        "metric.compute()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rig2ToPutQma"
      },
      "source": [
        "Let's now see some predictions ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mcptzb8X5l3",
        "outputId": "a2f512cd-d866-4a2d-a7e5-e5f0adc9fbe7"
      },
      "source": [
        "list(zip(references, predictions))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('THERE SHE OWNS A COTTAGE OR IT MAY BE A PIGSTYE FOR ALL I KNOW',\n",
              "  \"EOEOEMEODEMEO'OEO OEB E'ERUEOEEOEOEOEO<unk>EEPEMEZHEOMEZHEME<unk>OE'EUE<unk>E<unk>'EMEMO<unk>EUEOEEEEO<unk>E\"),\n",
              " ('THERE WERE NO SIGNS OF FALTERING ABOUT HER FLOW OF LANGUAGE',\n",
              "  'EEHOEOE<unk>EOEOEOE<s>EO<unk>OEOEEEM<unk>OE<s>DMOEHXE<unk>EXM<unk>EE<unk>E<unk>EU<unk>E<unk>EMOHEMEROE<unk>EEOOEE')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "Finally, we have reached an end to this notebook. But it's not an end of learning TensorFlow for speech-related tasks, this [repository](https://github.com/vasudevgupta7/gsoc-wav2vec2) contains some more amazing tutorials. Feel free to go through them. You can also refer [this repositary](https://github.com/tulasiram58827/TTS_TFLite) for some more amazing tutorials on speech-related tasks. In case you encountered any bug in this notebook, please create an issue [here](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBEm6caxYDyK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}