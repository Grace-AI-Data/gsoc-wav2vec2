{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 4,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.8.10 64-bit ('t5': conda)"
    },
    "interpreter": {
      "hash": "184c0bf4d405d4a36e719b504ff2a22c838d19108535bf816dff1a5aad495b87"
    },
    "colab": {
      "name": "wav2vec2-base-saved-model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vasudevgupta7/gsoc-wav2vec2/blob/export-v2/wav2vec2_base_saved_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndG8MjmJeicp"
      },
      "source": [
        "# How to train TensorFlow saved-model with extra head\n",
        "\n",
        "In this notebook, we will load the pre-trained wav2vec2 model from [TFHub](https://tfhub.dev) and will train it on [librispeech dataset](https://huggingface.co/datasets/librispeech_asr) by appending one extra head over the top of our pre-trained model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWk8nL6Ui-_0"
      },
      "source": [
        "## Setting Up\n",
        "\n",
        "Before diving into it, let's see what GPU we got using `nvidia-smi`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2DQV2hde_Vh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c230df0-ccc8-46aa-8e69-aa1aa3aca643"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jul 20 10:02:55 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 470.42.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   46C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8hBGUWT_mKkw"
      },
      "source": [
        "The following cell will clone my code repository ([`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2)) and will install all the dependencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seqTlMyeZvM4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2e8eb3e-2499-4c7b-8889-92d4f913b4d2"
      },
      "source": [
        "!git clone https://github.com/vasudevgupta7/gsoc-wav2vec2 --branch=export\n",
        "\n",
        "import sys\n",
        "import os\n",
        "\n",
        "os.chdir(\"gsoc-wav2vec2\")\n",
        "sys.path.append(\"src\")\n",
        "\n",
        "!pip3 install -qe ."
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'gsoc-wav2vec2'...\n",
            "remote: Enumerating objects: 371, done.\u001b[K\n",
            "remote: Counting objects: 100% (371/371), done.\u001b[K\n",
            "remote: Compressing objects: 100% (231/231), done.\u001b[K\n",
            "remote: Total 371 (delta 192), reused 299 (delta 130), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (371/371), 2.87 MiB | 6.96 MiB/s, done.\n",
            "Resolving deltas: 100% (192/192), done.\n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 43 kB 2.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.8 MB 58.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 133 kB 66.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 97 kB 8.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 170 kB 60.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n",
            "\u001b[?25h  Building wheel for python-Levenshtein (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFQDM1t8Z_XK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "326a8348-5938-45fa-8e73-a7e8345d2b8e"
      },
      "source": [
        "# This cell will be removed after model get exported to TFHub\n",
        "!wget https://huggingface.co/vasudevgupta/tf-wav2vec2-base/resolve/main/wav2vec2-base.tar.gz\n",
        "!tar -xf wav2vec2-base.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 10:03:09--  https://huggingface.co/vasudevgupta/tf-wav2vec2-base/resolve/main/wav2vec2-base.tar.gz\n",
            "Resolving huggingface.co (huggingface.co)... 15.197.130.34\n",
            "Connecting to huggingface.co (huggingface.co)|15.197.130.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/vasudevgupta/tf-wav2vec2-base/ba29ac5ff1f78271a6c9e6466cedd221e811b5ed58020337d238bc14512de9f3 [following]\n",
            "--2021-07-20 10:03:09--  https://cdn-lfs.huggingface.co/vasudevgupta/tf-wav2vec2-base/ba29ac5ff1f78271a6c9e6466cedd221e811b5ed58020337d238bc14512de9f3\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.225.143.41, 13.225.143.15, 13.225.143.61, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.225.143.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 387426816 (369M) [application/octet-stream]\n",
            "Saving to: ‘wav2vec2-base.tar.gz’\n",
            "\n",
            "wav2vec2-base.tar.g 100%[===================>] 369.48M  39.5MB/s    in 9.7s    \n",
            "\n",
            "2021-07-20 10:03:19 (38.0 MB/s) - ‘wav2vec2-base.tar.gz’ saved [387426816/387426816]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wvuJL8-f0zn5"
      },
      "source": [
        "## Model setup using `TFHub`\n",
        "\n",
        "We will start by importing all the important libraries & modules."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M3_fgx4eZvM7"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "from wav2vec2 import Wav2Vec2Config\n",
        "\n",
        "config = Wav2Vec2Config()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RuVyJshArx9Q"
      },
      "source": [
        "We will be loading pre-trained saved-model directly from TFHub. [`hub.load(...)`](https://www.tensorflow.org/hub/api_docs/python/hub/load) will download the pre-trained model first and will call [`tf.saved_model.load(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/load)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_5cjb_EZvM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9161ba3a-f7eb-47e6-845c-4e37e20a6a71"
      },
      "source": [
        "# TODO: update it to load from TFHub later\n",
        "loaded = hub.load(\"saved-model\")\n",
        "print(\"Available signatures are:\", list(loaded.signatures.keys()))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Available signatures are: ['infer', 'train']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0rVUxyWsS5f"
      },
      "source": [
        "We can see 2 signatures above, this is because while saving model with [`tf.saved_model.save(...)`](https://www.tensorflow.org/api_docs/python/tf/saved_model/save), `infer` & `train` signatures were provided (you can refer this [script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/export2hub.py)). We will be using the `train` signature for training this model on our downstream task. We will be wrapping our model signature with [`hub.KerasLayer`](https://www.tensorflow.org/hub/api_docs/python/hub/KerasLayer) to be able to freeze the pre-trained variables (Note: For this notebook, we will only train the extra head & not the pre-trained model).\n",
        "\n",
        "`lm_head` is a very simple dense layer having an output dimension of `vocab_size` as we want to predict probabilities of each token in the vocabulary at each time step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NO6QRC7KZvM9"
      },
      "source": [
        "pretrained_model = loaded.signatures[\"train\"]\n",
        "pretrained_model = hub.KerasLayer(pretrained_model, trainable=False)\n",
        "\n",
        "lm_head = tf.keras.layers.Dense(config.vocab_size)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPp18ZHRtnq-"
      },
      "source": [
        "Alright, let's define our forward pass by combining our pre-trained model & LM head. It's important to wrap it with `tf.function(...)` to be able to get performance benefits during training.\n",
        "\n",
        "Additionally, we will be passing `jit_compile=True` to compile (using XLA) our model graph on the accelerator (i.e GPU) & fuse many operations to get out-of-box performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f5M8YXXZvM_"
      },
      "source": [
        "@tf.function(jit_compile=True)\n",
        "def forward(batch):\n",
        "    return lm_head(pretrained_model(batch)[\"output_0\"])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqZuF43vuqsQ"
      },
      "source": [
        "## Setting training state\n",
        "\n",
        "In the following cell, we will be defining some of the hyper-parameters to be used in this notebook. `AUDIO_MAXLEN` is intentionally set to `246000` as the model signature only accepts static sequence length of `246000`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F_OWtO4uZvM-"
      },
      "source": [
        "BATCH_SIZE = 2\n",
        "LEARNING_RATE = 5e-5\n",
        "AUDIO_MAXLEN = 246000"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATQy1ZK3vFr7"
      },
      "source": [
        "In TensorFlow, model weights are build only when `model.__call__` is called for the first time, so the following cell will build the model weights for us.\n",
        "\n",
        "Further, we will be checking the total number of trainable variables to assert that the pre-trained model is frozen."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZgL5wyaXZvM-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14b3a90f-b35f-4a08-e816-e7109523ab5d"
      },
      "source": [
        "forward(tf.random.uniform(shape=(BATCH_SIZE, AUDIO_MAXLEN)))\n",
        "print(\"Number of trainable variables:\", len(list(pretrained_model.trainable_variables) + lm_head.trainable_variables))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of trainable variables: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQxxA4Fevp7m"
      },
      "source": [
        "Now, we need to define `loss_fn` and optimizer to be able to train the model. The following cell will do that for us. We will be using the `Adam` optimizer for simplicity. `CTCLoss` is a very common loss type that is used for tasks (like `ASR`) where input sub-parts can't be easily aligned with output sub-parts. You can read more about CTC-loss from this amazing [blog post](https://distill.pub/2017/ctc/).\n",
        "\n",
        "\n",
        "`CTCLoss` (from [`gsoc-wav2vec2`](https://github.com/vasudevgupta7/gsoc-wav2vec2) package) accepts 3 arguments: `config`, `model_input_shape` & `division_factor`. If `division_factor=1`, then loss will simply get summed, so pass `division_factor` accordingly to get mean over batch."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glDepVEHZvM_"
      },
      "source": [
        "from wav2vec2 import CTCLoss\n",
        "\n",
        "loss_fn = CTCLoss(config, (BATCH_SIZE, AUDIO_MAXLEN), division_factor=BATCH_SIZE)\n",
        "optimizer = tf.keras.optimizers.Adam(LEARNING_RATE)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mvTuOXpwsQe"
      },
      "source": [
        "## Setting up `tf.data.Dataset`\n",
        "\n",
        "\n",
        "We need to fetch some data to perform training. I have saved some part of `LibriSpeech tfrecords` in the  `HuggingFace Hub` ([see this](https://huggingface.co/datasets/vasudevgupta/gsoc-librispeech/tree/main)), so we will download those `tfrecords` using `wget`.\n",
        "\n",
        "Note: While converting [original LibriSpeech dataset](http://www.openslr.org/12) (i.e. `.flac` files) into `tfrecords`, some standard pre-processing is done. You can refer [this script](https://github.com/vasudevgupta7/gsoc-wav2vec2/blob/main/src/make_tfrecords.py) to know more on that."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I4kIEC77cBCM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46e99501-c871-4615-90a0-689e1e75be81"
      },
      "source": [
        "!wget https://huggingface.co/datasets/vasudevgupta/gsoc-librispeech/resolve/main/train-clean-100/train-clean-100-0.tfrecord -P /content/gsoc-wav2vec2/data/train/"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-20 10:04:26--  https://huggingface.co/datasets/vasudevgupta/gsoc-librispeech/resolve/main/train-clean-100/train-clean-100-0.tfrecord\n",
            "Resolving huggingface.co (huggingface.co)... 15.197.130.34\n",
            "Connecting to huggingface.co (huggingface.co)|15.197.130.34|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/datasets/vasudevgupta/gsoc-librispeech/df6dfb983f6514a98fc05d2ee219f57b1286589d61c1271bb10a0ed3effd6ae8 [following]\n",
            "--2021-07-20 10:04:26--  https://cdn-lfs.huggingface.co/datasets/vasudevgupta/gsoc-librispeech/df6dfb983f6514a98fc05d2ee219f57b1286589d61c1271bb10a0ed3effd6ae8\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 13.225.143.41, 13.225.143.15, 13.225.143.58, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|13.225.143.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 483730930 (461M) [application/octet-stream]\n",
            "Saving to: ‘/content/gsoc-wav2vec2/data/train/train-clean-100-0.tfrecord’\n",
            "\n",
            "train-clean-100-0.t 100%[===================>] 461.32M  31.8MB/s    in 15s     \n",
            "\n",
            "2021-07-20 10:04:42 (31.1 MB/s) - ‘/content/gsoc-wav2vec2/data/train/train-clean-100-0.tfrecord’ saved [483730930/483730930]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qbBpo74BcUuI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "958bcdce-84fb-4851-9807-6db7d4be2f15"
      },
      "source": [
        "ls /content/gsoc-wav2vec2/data/train/"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train-clean-100-0.tfrecord\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yBMiORo0xJD0"
      },
      "source": [
        "Following cell will setup `tf.data.Dataset` object for you using my `gsoc-wav2vec2` package."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jkIu_Wt4ZvNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c813f92f-16c2-4c9f-98ed-8691f2959bbc"
      },
      "source": [
        "from data_utils import LibriSpeechDataLoaderArgs, LibriSpeechDataLoader\n",
        "\n",
        "data_args = LibriSpeechDataLoaderArgs(\n",
        "    from_tfrecords=True,\n",
        "    tfrecords=[\"data/train/train-clean-100-0.tfrecord\"],\n",
        "    audio_maxlen=AUDIO_MAXLEN,\n",
        "    batch_size=BATCH_SIZE,\n",
        ")\n",
        "dataloader = LibriSpeechDataLoader(data_args)\n",
        "dataset = dataloader(seed=None)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading `vocab.json` from https://github.com/vasudevgupta7/gsoc-wav2vec2/raw/main/data/vocab.json ... DONE\n",
            "Reading tfrecords from ['data/train/train-clean-100-0.tfrecord'] ... Done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lqk2cs6LxVIh"
      },
      "source": [
        "Since this notebook is made for demonstration purposes, we will be taking first `num_batches` and will perform training over only that. You are encouraged to train on the whole dataset though."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z6GO5oYUxXtz"
      },
      "source": [
        "num_batches = 2\n",
        "dataset = dataset.take(num_batches)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJtSLgCvxvBG"
      },
      "source": [
        "## Training & Evaluation\n",
        "\n",
        "Let's define our `train_step` function now. There are 3 main steps in `train_step`: \n",
        "1. forward pass with variables tracking\n",
        "2. backward pass for calculating gradients\n",
        "3. variables update to minimize training loss\n",
        "\n",
        "All the trainable variables in the scope of `tf.GradientTape(...)` will get tracked during the forward pass. Further, `.gradient(...)` will help us find gradient of loss w.r.to those tracked variables & `.apply_gradients(...)` will update the trainable variables based on our `optimizer` defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RYI8ra_ZvNA"
      },
      "source": [
        "@tf.function\n",
        "def train_step(speech, labels):\n",
        "    with tf.GradientTape() as gtape:\n",
        "        speech = forward(speech)\n",
        "        loss = loss_fn(labels, speech)\n",
        "    trainable_variables = list(pretrained_model.trainable_variables) + lm_head.trainable_variables\n",
        "    grads = gtape.gradient(loss, trainable_variables)\n",
        "    optimizer.apply_gradients(zip(grads, trainable_variables))\n",
        "    return loss"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOuhcSPkytaN"
      },
      "source": [
        "Let's kick start training finally !!!\n",
        "\n",
        "We will iterate over our dataset (instance of `tf.data.Dataset`) and each batch will be fed to `train_step(...)` for calculating loss, gradients & updating parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUujvZn4ZvNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5056087-f136-4e09-9fc1-f53d51b310e9"
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "pbar = tqdm(dataset, total=num_batches)\n",
        "for speech, label in pbar:\n",
        "    loss = train_step(speech, label)\n",
        "    pbar.set_postfix(tr_loss=loss)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/2 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1408: alias_inplace_add (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_add, which offers the same functionality with well-defined read-write semantics.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/ctc_ops.py:1391: alias_inplace_update (from tensorflow.python.ops.inplace_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Prefer tf.tensor_scatter_nd_update, which offers the same functionality with well-defined read-write semantics.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:44<00:00, 22.47s/it, tr_loss=tf.Tensor(1931.6149, shape=(), dtype=float32)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJfPlTgezD0i"
      },
      "source": [
        "Let's compute loss over validation dataset using `eval_step(...)` defined in the following cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssWXWc7CZvNB"
      },
      "source": [
        "@tf.function\n",
        "def eval_step(speech, labels):\n",
        "    speech = forward(speech)\n",
        "    loss = loss_fn(labels, speech)\n",
        "    return loss"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq5tpZluzW8P"
      },
      "source": [
        "We are using the same dataset just for demonstration purposes. In general, we should use separate data (generally called `validation/dev` data) sampled before initiating training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQTFVjZghckJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d575c6-7f0b-43ff-98f8-ba26200f62e6"
      },
      "source": [
        "pbar = tqdm(dataset, total=num_batches)\n",
        "for speech, label in pbar:\n",
        "    loss = eval_step(speech, label)\n",
        "    pbar.set_postfix(val_loss=loss)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:14<00:00,  7.49s/it, val_loss=tf.Tensor(2013.693, shape=(), dtype=float32)]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7DXC757bztJc"
      },
      "source": [
        "Finally, we have reached an end to this notebook. But it's not an end of learning TensorFlow for speech-related tasks, this [repositary](https://github.com/vasudevgupta7/gsoc-wav2vec2) contains some more amazing tutorials. Feel free to go through them. In case you encounter any bug, please create an issue [here](https://github.com/vasudevgupta7/gsoc-wav2vec2/issues)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCxfNFFVh9LR"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}