{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  },
  "interpreter": {
   "hash": "07c4df7ea0886a603057dcbd29d886eaa26a0122c4b0c39e2937eba8f7497417"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.extend([\"../src\", \"/Users/vasudevgupta/local/transformers/src\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wav2vec2 import Wav2Vec2Processer, Wav2Vec2ForCTC\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Loading weights locally from `../wav2vec2-base-960h`\n",
      "Total number of loaded variables: 212\n"
     ]
    }
   ],
   "source": [
    "processor = Wav2Vec2Processer(is_tokenizer=False)\n",
    "tokenizer = Wav2Vec2Processer(is_tokenizer=True)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"../wav2vec2-base-960h\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 46797), dtype=float32, numpy=\n",
       "array([[ 0.0042721 , -0.00247198,  0.00764414, ..., -0.00247198,\n",
       "        -0.01596014, -0.02607627],\n",
       "       [-1.1425714 ,  1.2993361 ,  0.53832394, ..., -1.5862964 ,\n",
       "        -0.28949928, -1.0421019 ]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "batch, _ = tf.audio.decode_wav(tf.io.read_file(\"../data/sample.wav\"))\n",
    "batch = tf.transpose(batch, perm=(1, 0))\n",
    "batch = tf.concat([batch, tf.random.normal(batch.shape)], axis=0)\n",
    "batch = processor(batch)\n",
    "\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([2, 145, 32])"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "output[\"logits\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SHE HAD YOUR DUCK SUP AND GREASY WASHWATER AL YEAR'"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "source": [
    "tf_out = tf.argmax(output[\"logits\"], axis=-1)\n",
    "tf_out = tf.squeeze(tf_out).numpy()\n",
    "tokenizer.decode(tf_out[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC as HFWav2Vec2ForCTC\n",
    "MODEL_ID = \"facebook/wav2vec2-base-960h\"\n",
    "hf_model = HFWav2Vec2ForCTC.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer\n",
    "from transformers import Wav2Vec2Processor as HFWav2Vec2Processer\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID)\n",
    "hf_tokenizer =  Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)\n",
    "\n",
    "hf_processor = HFWav2Vec2Processer(feature_extractor=feature_extractor, tokenizer=hf_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 46797])"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "source": [
    "import torch\n",
    "sample, _ = tf.audio.decode_wav(tf.io.read_file(\"../data/sample.wav\"))\n",
    "sample = tf.transpose(sample, perm=(1, 0))\n",
    "\n",
    "hf_batch = torch.from_numpy(sample.numpy()).float()\n",
    "hf_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(torch.Size([1, 46797]),\n",
       " tensor([[ 0.0043, -0.0025,  0.0076,  ..., -0.0025, -0.0160, -0.0261]]))"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "hf_batch = hf_processor(hf_batch, return_tensors=\"pt\")['input_values'].squeeze(1)\n",
    "hf_batch.shape, hf_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "10.320303"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.max(hf_batch.numpy() - batch.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(145,)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hf_out = hf_model(hf_batch)\n",
    "hf_out = torch.argmax(hf_out[\"logits\"], dim=-1).numpy().squeeze()\n",
    "hf_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((145,), (2, 145))"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "hf_out.shape, tf_out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SHE HAD YOUR DUCK SUP AND GREASY WASHWATER AL YEAR'"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "tokenizer.decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'SHE HAD YOUR DUCK SUP AND GREASY WASHWATER ALL YEAR'"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "hf_tokenizer.decode(hf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'S',\n",
       " 'S',\n",
       " 'H',\n",
       " '<pad>',\n",
       " 'E',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " 'H',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'A',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'D',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " 'Y',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'O',\n",
       " 'U',\n",
       " '<pad>',\n",
       " 'R',\n",
       " '|',\n",
       " '|',\n",
       " 'D',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'U',\n",
       " '<pad>',\n",
       " 'C',\n",
       " '<pad>',\n",
       " 'K',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " '<pad>',\n",
       " 'S',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'U',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'P',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " 'A',\n",
       " 'N',\n",
       " '<pad>',\n",
       " 'D',\n",
       " '|',\n",
       " '|',\n",
       " 'G',\n",
       " '<pad>',\n",
       " 'R',\n",
       " '<pad>',\n",
       " 'E',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'A',\n",
       " '<pad>',\n",
       " 'S',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'Y',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " 'W',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'A',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'S',\n",
       " '<pad>',\n",
       " 'H',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'W',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'A',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'T',\n",
       " 'E',\n",
       " 'E',\n",
       " 'R',\n",
       " '|',\n",
       " '|',\n",
       " '|',\n",
       " 'A',\n",
       " 'L',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'L',\n",
       " '|',\n",
       " '|',\n",
       " '|',\n",
       " '|',\n",
       " 'Y',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " 'E',\n",
       " 'A',\n",
       " 'A',\n",
       " 'R',\n",
       " '<pad>',\n",
       " '|',\n",
       " '|',\n",
       " '|',\n",
       " '|',\n",
       " '<pad>',\n",
       " '<pad>',\n",
       " '<pad>']"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "[tokenizer.id_to_token_mapping[t] for t in hf_out.tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "np.max(hf_out.squeeze() - tf_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2CTCTokenizer, Wav2Vec2ForCTC as HFWav2Vec2ForCTC\n",
    "MODEL_ID = \"facebook/wav2vec2-base-960h\"\n",
    "model = HFWav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "tokenizer = Wav2Vec2CTCTokenizer.from_pretrained(MODEL_ID)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutomaticSpeechRecognitionPipeline\n",
    "agent = AutomaticSpeechRecognitionPipeline(feature_extractor, model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "HMMMMMMMMMM\n",
      "HIIIIII\n",
      "{'input_values': tensor([[ 0.0043, -0.0025,  0.0076,  ..., -0.0025, -0.0160, -0.0261]])}\n",
      "tensor([[[ 13.0021, -26.9057, -26.6217,  ...,  -6.4053,  -8.6990,  -7.6950],\n",
      "         [ 13.0199, -26.8902, -26.6022,  ...,  -6.4001,  -8.7187,  -7.7267],\n",
      "         [ 13.0194, -26.8902, -26.6021,  ...,  -6.4002,  -8.7183,  -7.7272],\n",
      "         ...,\n",
      "         [ 13.0190, -26.8911, -26.6032,  ...,  -6.4001,  -8.7187,  -7.7278],\n",
      "         [ 13.0168, -26.8930, -26.6053,  ...,  -6.4000,  -8.7142,  -7.7307],\n",
      "         [ 13.0183, -26.8919, -26.6039,  ...,  -6.3995,  -8.7190,  -7.7278]]],\n",
      "       grad_fn=<AddBackward0>)\n",
      "TOKENS\n",
      "torch.Size([145]) tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 12, 11,  0,  5,  0,  0,\n",
      "         4,  4, 11,  0,  0,  7,  0,  0, 14,  0,  4,  4, 22,  0,  0,  8, 16,  0,\n",
      "        13,  4,  4, 14,  0,  0,  0, 16,  0, 19,  0, 26,  0,  0,  4,  4,  0, 12,\n",
      "         0,  0,  0,  0,  0, 16,  0,  0, 23,  0,  4,  4,  7,  9,  0, 14,  4,  4,\n",
      "        21,  0, 13,  0,  5,  0,  0,  7,  0, 12,  0,  0,  0,  0,  0, 22,  0,  4,\n",
      "         4, 18,  0,  0,  0,  0,  7,  0,  0, 12,  0, 11,  0,  0,  0, 18,  0,  0,\n",
      "         0,  7,  0,  0,  6,  5,  5, 13,  4,  4,  4,  7, 15,  0,  0,  0, 15,  4,\n",
      "         4,  4,  4, 22,  0,  0,  0,  5,  7,  7, 13,  0,  4,  4,  4,  4,  0,  0,\n",
      "         0])\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'text': 'SHE HAD YOUR DUCK SUP AND GREASY WASHWATER ALL YEAR'}"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "import soundfile as sf\n",
    "speech, _ = sf.read(\"../data/sample.wav\")\n",
    "agent(speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0, 12, 12, 11,  0,  5,  0,\n",
       "        0,  4,  4, 11,  0,  0,  7,  0,  0, 14,  0,  4,  4, 22,  0,  0,  8,\n",
       "       16,  0, 13,  4,  4, 14,  0,  0,  0, 16,  0, 19,  0, 26,  0,  0,  4,\n",
       "        4,  0, 12,  0,  0,  0,  0,  0, 16,  0,  0, 23,  0,  4,  4,  7,  9,\n",
       "        0, 14,  4,  4, 21,  0, 13,  0,  5,  0,  0,  7,  0, 12,  0,  0,  0,\n",
       "        0,  0, 22,  0,  4,  4, 18,  0,  0,  0,  0,  7,  0,  0, 12,  0, 11,\n",
       "        0,  0,  0, 18,  0,  0,  0,  7,  0,  0,  6,  5,  5, 13,  4,  4,  4,\n",
       "        7, 15,  0,  0,  0, 15,  4,  4,  4,  4, 22,  0,  0,  0,  5,  7,  7,\n",
       "       13,  0,  4,  4,  4,  4,  0,  0,  0])"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "hf_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 46797), dtype=float32, numpy=\n",
       "array([[ 0.0042721 , -0.00247198,  0.00764414, ..., -0.00247198,\n",
       "        -0.01596014, -0.02607627],\n",
       "       [ 0.13727087, -0.41194385, -0.3056962 , ..., -1.6625165 ,\n",
       "        -1.0761024 , -0.70854855]], dtype=float32)>"
      ]
     },
     "metadata": {},
     "execution_count": 33
    }
   ],
   "source": [
    "processor(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "It is strongly recommended to pass the ``sampling_rate`` argument to this function.Failing to do so can result in silent errors that might be hard to debug.\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'input_values': array([[ 0.0042721 , -0.00247198,  0.00764414, ..., -0.00247198,\n",
       "        -0.01596014, -0.02607627],\n",
       "       [ 0.13727087, -0.41194386, -0.3056962 , ..., -1.66251652,\n",
       "        -1.07610243, -0.70854858]])}"
      ]
     },
     "metadata": {},
     "execution_count": 40
    }
   ],
   "source": [
    "feature_extractor(batch.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2, 46797), dtype=float32, numpy=\n",
       " array([[ 3.0517578e-05, -3.0517578e-05,  6.1035156e-05, ...,\n",
       "         -3.0517578e-05, -1.5258789e-04, -2.4414062e-04],\n",
       "        [ 1.4000420e-01, -4.0828085e-01, -3.0221304e-01, ...,\n",
       "         -1.6567366e+00, -1.0713152e+00, -7.0438349e-01]], dtype=float32)>,\n",
       " tensor([[ 3.0518e-05, -3.0518e-05,  6.1035e-05,  ..., -3.0518e-05,\n",
       "          -1.5259e-04, -2.4414e-04],\n",
       "         [ 1.4000e-01, -4.0828e-01, -3.0221e-01,  ..., -1.6567e+00,\n",
       "          -1.0713e+00, -7.0438e-01]]))"
      ]
     },
     "metadata": {},
     "execution_count": 36
    }
   ],
   "source": [
    "batch, hf_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}